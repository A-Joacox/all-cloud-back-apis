import psycopg2
import pandas as pd
import boto3
from datetime import datetime
import json
import os
from dotenv import load_dotenv
import logging
from botocore.exceptions import ClientError, NoCredentialsError

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cargar variables de entorno
load_dotenv()

class PostgreSQLToS3Academy:
    def __init__(self):
        # ConfiguraciÃ³n PostgreSQL
        self.postgres_config = {
            'host': os.getenv('POSTGRES_HOST', 'localhost'),
            'port': int(os.getenv('POSTGRES_PORT', 5432)),
            'user': os.getenv('POSTGRES_USER', 'postgres'),
            'password': os.getenv('POSTGRES_PASSWORD', 'cinema_password'),
            'database': os.getenv('POSTGRES_DATABASE', 'cinema_reservations')
        }
        
        # ConfiguraciÃ³n AWS Academy (con Session Token)
        self.s3_bucket = os.getenv('S3_BUCKET', 'cinema-analytics-data')
        self.aws_region = os.getenv('AWS_DEFAULT_REGION', 'us-east-1')
        
        # Credenciales AWS Academy
        aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')
        aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')
        aws_session_token = os.getenv('AWS_SESSION_TOKEN')
        
        if not all([aws_access_key, aws_secret_key, aws_session_token]):
            logger.error("âŒ Faltan credenciales AWS Academy")
            logger.error("Necesitas: ACCESS_KEY, SECRET_KEY y SESSION_TOKEN")
            raise ValueError("Credenciales incompletas")
        
        # Cliente S3 con Session Token
        try:
            self.s3_client = boto3.client(
                's3',
                region_name=self.aws_region,
                aws_access_key_id=aws_access_key,
                aws_secret_access_key=aws_secret_key,
                aws_session_token=aws_session_token
            )
            
            # Probar credenciales
            self.s3_client.list_buckets()
            logger.info("âœ… Credenciales AWS Academy vÃ¡lidas")
            
        except Exception as e:
            logger.error(f"âŒ Error con credenciales AWS Academy: {e}")
            raise

    def create_s3_bucket_if_not_exists(self):
        """Crear bucket S3 si no existe"""
        try:
            self.s3_client.head_bucket(Bucket=self.s3_bucket)
            logger.info(f"âœ… Bucket {self.s3_bucket} ya existe")
        except ClientError as e:
            error_code = int(e.response['Error']['Code'])
            if error_code == 404:
                try:
                    # Crear bucket
                    if self.aws_region == 'us-east-1':
                        self.s3_client.create_bucket(Bucket=self.s3_bucket)
                    else:
                        self.s3_client.create_bucket(
                            Bucket=self.s3_bucket,
                            CreateBucketConfiguration={'LocationConstraint': self.aws_region}
                        )
                    logger.info(f"âœ… Bucket {self.s3_bucket} creado exitosamente")
                except ClientError as create_error:
                    logger.error(f"âŒ Error creando bucket: {create_error}")
                    raise
            else:
                logger.error(f"âŒ Error accediendo al bucket: {e}")
                raise

    def connect_to_postgresql(self):
        """Conectar a PostgreSQL"""
        try:
            connection = psycopg2.connect(**self.postgres_config)
            logger.info("âœ… ConexiÃ³n exitosa a PostgreSQL")
            return connection
        except psycopg2.Error as err:
            logger.error(f"âŒ Error conectando a PostgreSQL: {err}")
            raise

    def extract_table_data(self, connection, table_name, limit=None):
        """Extraer datos de una tabla especÃ­fica"""
        try:
            query = f"SELECT * FROM {table_name}"
            if limit:
                query += f" LIMIT {limit}"
            
            df = pd.read_sql(query, connection)
            logger.info(f"ğŸ“Š Datos extraÃ­dos de {table_name}: {len(df)} registros")
            return df
        except Exception as e:
            logger.error(f"âŒ Error extrayendo datos de {table_name}: {e}")
            return None

    def upload_dataframe_to_s3(self, df, s3_key, format_type='csv'):
        """Subir DataFrame a S3"""
        try:
            if format_type == 'csv':
                buffer = df.to_csv(index=False)
                content_type = 'text/csv'
            elif format_type == 'json':
                # Convert datetime objects to string for JSON serialization
                df_json = df.copy()
                for col in df_json.columns:
                    if df_json[col].dtype == 'datetime64[ns]':
                        df_json[col] = df_json[col].dt.isoformat()
                buffer = df_json.to_json(orient='records', indent=2)
                content_type = 'application/json'
            else:
                raise ValueError(f"Formato no soportado: {format_type}")
            
            self.s3_client.put_object(
                Bucket=self.s3_bucket,
                Key=s3_key,
                Body=buffer,
                ContentType=content_type
            )
            
            logger.info(f"âœ… Archivo subido: s3://{self.s3_bucket}/{s3_key}")
            return True
        except Exception as e:
            logger.error(f"âŒ Error subiendo archivo: {e}")
            return False

    def extract_and_upload_all_tables(self):
        """Extraer y subir todas las tablas del sistema de reservas"""
        connection = self.connect_to_postgresql()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Tablas principales del sistema de reservas
        tables = ['users', 'reservations', 'reserved_seats', 'payments']
        
        try:
            # Crear bucket si no existe
            self.create_s3_bucket_if_not_exists()
            
            uploaded_files = []
            
            for table in tables:
                logger.info(f"ğŸ”„ Procesando tabla: {table}")
                
                # Extraer datos
                df = self.extract_table_data(connection, table)
                
                if df is not None and not df.empty:
                    # Subir como CSV
                    s3_key_csv = f"postgresql-data/reservations/{table}_{timestamp}.csv"
                    if self.upload_dataframe_to_s3(df, s3_key_csv, 'csv'):
                        uploaded_files.append(s3_key_csv)
                    
                    # Subir como JSON
                    s3_key_json = f"postgresql-data/reservations/{table}_{timestamp}.json"
                    if self.upload_dataframe_to_s3(df, s3_key_json, 'json'):
                        uploaded_files.append(s3_key_json)
                    
                    # Mostrar muestra de datos
                    print(f"\nğŸ“‹ Muestra de datos de {table}:")
                    print(df.head())
                else:
                    logger.warning(f"âš ï¸ No hay datos en la tabla {table}")
            
            # Crear un archivo de metadatos
            metadata = {
                "extraction_date": datetime.now().isoformat(),
                "database_type": "postgresql",
                "database_name": self.postgres_config['database'],
                "tables_exported": tables,
                "files_uploaded": uploaded_files,
                "total_files": len(uploaded_files)
            }
            
            metadata_key = f"postgresql-data/reservations/metadata_{timestamp}.json"
            metadata_buffer = json.dumps(metadata, indent=2)
            
            self.s3_client.put_object(
                Bucket=self.s3_bucket,
                Key=metadata_key,
                Body=metadata_buffer,
                ContentType='application/json'
            )
            
            logger.info(f"âœ… Metadata creado: s3://{self.s3_bucket}/{metadata_key}")
            
            # Verificar archivos subidos
            objects = self.s3_client.list_objects_v2(
                Bucket=self.s3_bucket,
                Prefix="postgresql-data/reservations/"
            )
            
            if 'Contents' in objects:
                logger.info(f"ğŸ“‚ Total de archivos en S3: {len(objects['Contents'])}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error durante la extracciÃ³n: {e}")
            return False
        finally:
            connection.close()

    def extract_and_upload_test(self):
        """Test de extracciÃ³n y subida con una tabla pequeÃ±a"""
        connection = self.connect_to_postgresql()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            # Crear bucket si no existe
            self.create_s3_bucket_if_not_exists()
            
            # Consulta de prueba (usuarios)
            df = self.extract_table_data(connection, 'users', limit=3)
            
            if df is not None and not df.empty:
                logger.info(f"ğŸ“Š Datos extraÃ­dos: {len(df)} registros")
                print(df.head())
                
                # Subir a S3
                s3_key = f"postgresql-data/test/users_test_{timestamp}.csv"
                if self.upload_dataframe_to_s3(df, s3_key, 'csv'):
                    logger.info(f"âœ… Test exitoso!")
                    return True
            else:
                logger.warning("âš ï¸ No hay datos en la tabla users")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Error en test: {e}")
            return False
        finally:
            connection.close()

def main():
    """FunciÃ³n principal"""
    try:
        print("ğŸš€ Iniciando ingesta PostgreSQL con AWS Academy...")
        
        ingestion = PostgreSQLToS3Academy()
        
        # Detectar si se ejecuta desde run-all-ingesta (modo automÃ¡tico)
        import sys
        auto_mode = len(sys.argv) > 1 and sys.argv[1] == 'auto'
        
        if auto_mode:
            # Modo automÃ¡tico: ejecutar extracciÃ³n completa
            print("\nğŸ“¦ Modo automÃ¡tico: Ejecutando extracciÃ³n completa...")
            result = ingestion.extract_and_upload_all_tables()
        else:
            # Modo interactivo
            choice = input("\nÂ¿QuÃ© deseas hacer?\n1. Test rÃ¡pido\n2. ExtracciÃ³n completa\nElige (1/2): ").strip()
            
            if choice == '1':
                print("\nğŸ§ª Ejecutando test...")
                result = ingestion.extract_and_upload_test()
            elif choice == '2':
                print("\nğŸ“¦ Ejecutando extracciÃ³n completa...")
                result = ingestion.extract_and_upload_all_tables()
            else:
                print("âŒ OpciÃ³n no vÃ¡lida")
                return
        
        if result:
            print("\nâœ… Â¡Ingesta completada exitosamente!")
            print("ğŸ‰ Los datos de PostgreSQL han sido subidos a S3")
        else:
            print("\nâŒ La ingesta fallÃ³")
        
    except Exception as e:
        logger.error(f"âŒ Error: {e}")
        print("\nğŸ”§ Para solucionar:")
        print("1. Verifica la conexiÃ³n a PostgreSQL")
        print("2. Copia las 3 lÃ­neas de AWS Academy CLI")
        print("3. PÃ©galas en terminal (export...)")
        print("4. O actualiza el archivo .env con SESSION_TOKEN")

if __name__ == "__main__":
    main()